{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime Twitter Sentiment Analysis with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/Raymond/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pydotplus as pydot\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Input, Dense, Embedding, MaxPooling1D\n",
    "from keras.layers import SpatialDropout1D, concatenate, Dropout, BatchNormalization\n",
    "from keras.layers import LSTM, GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, SeparableConv1D\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The Twitter Sentiment Analysis Dataset contains 1,578,627 classified tweets, each row is marked as 1 for positive sentiment and 0 for negative sentiment.  The dataset is based on data from the following two sources:\n",
    "\n",
    "* University of Michigan Sentiment Analysis competition on Kaggle\n",
    "* Twitter Sentiment Corpus by Niek Sanders\n",
    "\n",
    "It can be found at http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/twitter_sentiment_dataset.csv', encoding='latin1', usecols=['Sentiment', 'SentimentText'])\n",
    "data.columns = ['sentiment', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1578614, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "First, we need to clean the text by removing all special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(tweet):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].map(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that I also tried to remove stop words from the text, but that actually resulted a worse accuracy for my model.\n",
    "\n",
    "Now that the data is clean, let's prepare the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1420752,) (157862,) (1420752,) (157862,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], \n",
    "                                                    data['sentiment'], \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify=data['sentiment'])\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use Keras on text data, we need to tokenize the text first. This can be done using the Tokenizer function and specifying a max number of words we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 100000\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "\n",
    "tokenizer.fit_on_texts(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 288603 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('There are {} unique tokens.'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Embedding \n",
    "\n",
    "There are various precomputed databases of word embeddings that you can download and use in a Keras Embedding layer. Word2vec is one of them. Another popular one is called Global Vectors for Word Representation (GloVe, https://nlp.stanford.edu/projects/glove), which was developed by Stanford researchers in 2014. This embedding technique is based on factorizing a matrix of word co-occurrence statistics.\n",
    "\n",
    "For my model, I use the Pretrained Twitter Word Vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model..\n",
      "Done. 1193513  words loaded!\n"
     ]
    }
   ],
   "source": [
    "embed_size = 200\n",
    "\n",
    "def loadGloveModel(glove_file, embed_size=200):\n",
    "    print(\"Loading Glove Model..\")\n",
    "    file = open(glove_file, 'r')\n",
    "    word_embedding_dict = {}\n",
    "    \n",
    "    for line in file:\n",
    "        word_embedding = line.split()\n",
    "        word = word_embedding[0]\n",
    "        embedding = np.asarray([float(val) for val in word_embedding[1:]], dtype='float32')\n",
    "        word_embedding_dict[word] = embedding\n",
    "    \n",
    "    # Remove words with wrong embedding size\n",
    "    # Iterate through a copy to avoid error\n",
    "    for k, v in word_embedding_dict.copy().items():\n",
    "        if len(v) != embed_size:\n",
    "            word_embedding_dict.pop(k)\n",
    "            \n",
    "    print(\"Done.\",len(word_embedding_dict),\" words loaded!\")\n",
    "    \n",
    "    file.close()\n",
    "    return word_embedding_dict\n",
    "\n",
    "glove_embedding_dict = loadGloveModel('glove.twitter.27B.200d.txt', embed_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of the embedding for the word 'heart'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.068954 , -0.064559 , -0.2532   ,  0.24135  ,  0.34572  ,\n",
       "       -0.1799   ,  0.56555  , -0.12854  , -0.32679  , -0.24896  ,\n",
       "        0.23996  , -0.19216  , -1.4679   ,  0.41168  , -0.48718  ,\n",
       "        0.073335 , -0.16831  , -0.43334  ,  0.5277   , -0.2179   ,\n",
       "       -0.087065 ,  0.21645  ,  0.10003  ,  0.29946  ,  0.23227  ,\n",
       "        0.88475  , -0.51618  ,  0.13294  ,  0.49017  ,  0.48222  ,\n",
       "       -0.33084  , -0.29034  ,  0.42365  ,  0.42157  ,  0.073902 ,\n",
       "       -0.38198  , -0.20283  , -0.54664  , -0.24354  ,  0.40618  ,\n",
       "       -0.54074  ,  0.16636  ,  0.45591  ,  0.26943  ,  0.0058961,\n",
       "        0.15221  ,  0.5307   ,  0.20654  ,  0.11243  ,  0.20151  ,\n",
       "       -0.2208   ,  0.45178  ,  0.16479  ,  0.095516 ,  0.45435  ,\n",
       "       -0.31694  ,  0.45188  ,  0.58922  , -0.071485 ,  0.050712 ,\n",
       "       -0.11368  ,  0.12427  ,  0.015246 ,  0.074834 ,  0.24083  ,\n",
       "       -0.14761  ,  0.59176  ,  0.3154   ,  0.0070812,  0.19171  ,\n",
       "       -0.14031  , -0.16693  ,  0.069561 , -0.020206 ,  0.45434  ,\n",
       "       -0.49432  , -0.60644  , -0.40874  , -0.15706  , -0.412    ,\n",
       "        0.39113  ,  0.017955 ,  0.84421  , -0.35203  , -0.69117  ,\n",
       "       -0.12042  , -0.27706  ,  0.76441  , -0.54839  ,  0.11421  ,\n",
       "       -0.45311  , -0.33062  ,  1.0277   , -0.57999  , -0.36173  ,\n",
       "       -0.39483  ,  0.37464  , -0.6439   , -0.45431  ,  0.52209  ,\n",
       "       -0.14871  ,  0.2148   ,  0.38556  ,  0.024187 ,  0.19685  ,\n",
       "        0.47384  ,  0.02055  , -0.059706 , -0.51102  , -0.084938 ,\n",
       "       -0.26217  ,  0.43572  ,  0.40166  ,  0.30958  , -0.6126   ,\n",
       "        0.24247  ,  0.57128  ,  0.92565  , -0.21597  , -0.21078  ,\n",
       "        0.058566 ,  0.19522  , -0.0501   ,  0.25422  , -0.59405  ,\n",
       "        0.94742  , -0.30414  , -0.42465  ,  0.077589 ,  0.34663  ,\n",
       "       -0.14646  ,  0.38593  ,  0.057665 ,  0.52025  ,  0.27786  ,\n",
       "       -0.14034  , -0.015052 , -0.4417   , -0.14804  , -0.032525 ,\n",
       "       -0.4672   ,  0.44433  , -0.15105  , -0.63647  , -0.22265  ,\n",
       "       -0.085928 , -0.42632  ,  0.38853  , -0.47809  , -0.20978  ,\n",
       "        0.54666  , -0.36585  , -4.8082   ,  0.12742  , -0.62146  ,\n",
       "        0.024324 , -0.26606  , -0.16766  ,  0.11203  ,  0.010676 ,\n",
       "        0.99502  , -0.67296  , -0.44845  , -0.32214  , -0.0092586,\n",
       "       -0.065635 ,  0.52679  ,  0.37324  , -0.56631  ,  0.23496  ,\n",
       "        0.31414  , -0.056544 , -0.28738  , -0.48415  ,  0.40473  ,\n",
       "        0.40763  , -0.26561  , -0.26129  ,  0.37716  , -0.051828 ,\n",
       "       -0.089598 ,  0.011927 , -0.1359   , -0.11249  ,  0.17483  ,\n",
       "       -0.48943  ,  0.29709  , -0.24435  ,  0.49868  ,  0.19024  ,\n",
       "        0.11642  ,  0.25956  , -0.3007   , -0.6191   ,  0.41202  ,\n",
       "       -0.30678  ,  0.56128  ,  0.016853 ,  0.006461 , -0.44972  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding_dict['heart']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I build an embedding matrix to be used in my Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genEmbeddingMatrix(embedding_dict, tokenizer, max_words=100000, embed_size=200):\n",
    "    all_embeddings = np.stack(list(embedding_dict.values()))\n",
    "    embedding_mean, embedding_std = all_embeddings.mean(), all_embeddings.std()\n",
    "    embedding_matrix = np.random.normal(embedding_mean, embedding_std, (max_words, embed_size))\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if word in embedding_dict.keys() and i < max_words:\n",
    "            embedding_matrix[i] = embedding_dict[word]\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = genEmbeddingMatrix(glove_embedding_dict, tokenizer, max_words=MAX_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use Keras for text and sequences, I first have to preprocess the text.  This can be done with Keras' Tokenizer class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lt This is the way i feel right now'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[159, 28, 9, 3, 131, 1, 110, 117, 29]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([X_train[15]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words are mapped into a list of integers.  The most frequent words are taken into account first.  For example, it can be seen that the word 'i' corresponds to number 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the tweets are a list of integers, we need to make sure the lists are all the same size in order to stack them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 35\n",
    "padded_train_sequences = pad_sequences(train_sequences, maxlen=MAX_LENGTH)\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   162,   356,   224],\n",
       "       [    0,     0,     0, ...,   879,  1656,   661],\n",
       "       [    0,     0,     0, ...,     0,   153,  6543],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,  1504,  1469, 26172],\n",
       "       [    0,     0,     0, ...,    55,    94,   433],\n",
       "       [    0,     0,     0, ...,   193,    13,     6]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1420752, 35)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "## Embedding Layer\n",
    "* In an embedding, words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space.\n",
    "* The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used.\n",
    "* The position of a word in the learned vector space is referred to as its embedding.\n",
    "* Keras offers an Embedding layer that can be used for neural networks on text data. It requires that the input data be integer encoded, so that each word is represented by a unique integer.  The Embedding layer is normally initialized with random weights and will learn an embedding for all of the words in the training dataset.\n",
    "* In this case I use the pretrained **GLoVe** embedding matrix.\n",
    "* Setting trainable to True yielded better results for me.\n",
    "\n",
    "\n",
    "## Bidirectional LSTM\n",
    "* A bidirectional RNN exploits the idea of that an RNN trained on reversed sequences will learn different representations than one trained on the original sequences.\n",
    "* It looks at its input sequence both ways, obtaining potentially richer representations and capturing patterns that may have been missed by the chronological-order version alone.\n",
    "* Recurrent dropout is used to prevent overfitting.\n",
    "\n",
    "## SeparableConv1D\n",
    "* 1D convolution layers can recognize local patterns in a sequence. \n",
    "* Because the same input transformation is performed on every patch, a pattern learned at a certain position in a sentence can later be recognized at a different position\n",
    "* I opted for a Depthwise Separable Convolution Layer because it separates the learning of spatial features and the learning of channel-wise features.\n",
    "\n",
    "## Combining RNN & CNN\n",
    "Source: <a href=http://konukoii.com/blog/2018/02/19/twitter-sentiment-analysis-using-combined-lstm-cnn-models/> here</a>\n",
    "* The idea behind combining an RNN and a CNN is that the output tokens of the RNN will store information not only of the initial token, but also any previous tokens; In other words, the LSTM layer is generating a new encoding for the original input. The output of the LSTM layer is then fed into a convolution layer which we expect will extract local features. \n",
    "* Finally the convolution layerâ€™s output will be pooled to a smaller dimension and ultimately outputted as either a positive or negative label.\n",
    "\n",
    "## Batch Normalization\n",
    "* Batch normalization is a type of layer (BatchNormalization in Keras) introduced in 2015 by Ioffe and Szegedy. It can adaptively normalize data even as the mean and variance change over time during training. It works by internally maintaining an exponential moving average of the batch-wise mean and variance of the data seen during training. \n",
    "* Data is usually normalized before being inputted into a model, but normalization should also be taken of after every transformational layer in a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_cnn_model(max_words, embedding_dim, embedding_matrix=None):\n",
    "    if embedding_matrix is None:\n",
    "        embedding_matrix = np.random.random((max_words, embedding_dim))\n",
    "    \n",
    "    inp = Input(shape=(MAX_LENGTH, ))\n",
    "    \n",
    "    # glove embedding\n",
    "    x = Embedding(input_dim=MAX_WORDS, output_dim=embedding_dim, input_length=MAX_LENGTH, \n",
    "                  weights=[embedding_matrix], trainable=True)(inp)\n",
    "    x = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.2))(x)\n",
    "    x = SeparableConv1D(128, kernel_size=3, padding=\"same\", kernel_initializer=\"random_uniform\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    concat = concatenate([avg_pool, max_pool])\n",
    "    \n",
    "    outp = Dense(1, activation=\"sigmoid\")(concat)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='RMSprop',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "lstm_cnn_model = build_lstm_cnn_model(max_words=MAX_WORDS, embedding_dim=200, embedding_matrix=embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network can be visualized with Keras' plot_model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(lstm_cnn_model, to_file='rnn.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn](rnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1420752 samples, validate on 157862 samples\n",
      "Epoch 1/2\n",
      "1420752/1420752 [==============================] - 467s 329us/step - loss: 0.3982 - acc: 0.8184 - val_loss: 0.3720 - val_acc: 0.8341\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.83408, saving model to ./models/weights-improvement-01-0.8341.hdf5\n",
      "Epoch 2/2\n",
      "1420752/1420752 [==============================] - 461s 325us/step - loss: 0.3576 - acc: 0.8412 - val_loss: 0.3618 - val_acc: 0.8400\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.83408 to 0.83999, saving model to ./models/weights-improvement-02-0.8400.hdf5\n"
     ]
    }
   ],
   "source": [
    "filepath=\"./models/weights-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 2\n",
    "\n",
    "history = lstm_cnn_model.fit(x=padded_train_sequences, \n",
    "                             y=y_train, \n",
    "                             validation_data=(padded_test_sequences, y_test), \n",
    "                             batch_size=batch_size, \n",
    "                             callbacks=[checkpoint], \n",
    "                             epochs=epochs, \n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157862/157862 [==============================] - 9s 58us/step\n"
     ]
    }
   ],
   "source": [
    "best_lstm_cnn_model = load_model('./models/weights-improvement-02-{:0.4f}.hdf5'.format(checkpoint.best))\n",
    "\n",
    "y_pred_rnn_cnn = best_lstm_cnn_model.predict(padded_test_sequences, verbose=1, batch_size=2048)\n",
    "\n",
    "y_pred_rnn_cnn = pd.DataFrame(y_pred_rnn_cnn, columns=['prediction'])\n",
    "y_pred_rnn_cnn['prediction'] = y_pred_rnn_cnn['prediction'].map(lambda p: 1 if p >= 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[65959 12885]\n",
      " [12375 66643]]\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.84      0.84     78844\n",
      "          1       0.84      0.84      0.84     79018\n",
      "\n",
      "avg / total       0.84      0.84      0.84    157862\n",
      "\n",
      "ROC AUC score: 0.8399830685925739\n",
      "Accuracy Score: 0.8399868239348292\n"
     ]
    }
   ],
   "source": [
    "def printClassificationErrors(y_test, y_pred):\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('ROC AUC score: {}'.format(roc_auc_score(y_test, y_pred)))\n",
    "    print('Accuracy Score: {}'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "printClassificationErrors(y_test, y_pred_rnn_cnn['prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a validation accuracy of about 84%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grabbing Tweet Data using the Tweepy API\n",
    "\n",
    "We can grab live twitter data using Twitter's Tweepy API.  I created a subclass of the StreamListener class in order to add parameters like a time limit, number of tweets, whether to grab retweets or not, and filter words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.tweepy_streaming import saveTweepyTweets\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import json\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweepy_listener = saveTweepyTweets(time_limit=240, \n",
    "                                   num_of_tweets=50, \n",
    "                                   save_file='twitter_stream_data.json', \n",
    "                                   retweets=False, \n",
    "                                   filter_set=config.FILTERED_WORDS)\n",
    "auth = OAuthHandler(config.CONSUMER_KEY, config.CONSUMER_SECRET)\n",
    "auth.set_access_token(config.ACCESS_TOKEN, config.ACCESS_TOKEN_SECRET)\n",
    "stream = Stream(auth=auth, listener=tweepy_listener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tweet #1...\n",
      "Getting tweet #2...\n",
      "Getting tweet #3...\n",
      "Getting tweet #4...\n",
      "Getting tweet #5...\n",
      "Getting tweet #6...\n",
      "Getting tweet #7...\n",
      "Getting tweet #8...\n",
      "Getting tweet #9...\n",
      "Getting tweet #10...\n",
      "Getting tweet #11...\n",
      "Getting tweet #12...\n",
      "Getting tweet #13...\n",
      "Getting tweet #14...\n",
      "Getting tweet #15...\n",
      "Getting tweet #16...\n",
      "Filtering...\n",
      "Getting tweet #17...\n",
      "Getting tweet #18...\n",
      "Getting tweet #19...\n",
      "Filtering...\n",
      "Getting tweet #20...\n",
      "Getting tweet #21...\n",
      "Filtering...\n",
      "Getting tweet #22...\n",
      "Getting tweet #23...\n",
      "Getting tweet #24...\n",
      "Getting tweet #25...\n",
      "Filtering...\n",
      "Getting tweet #26...\n",
      "Getting tweet #27...\n",
      "Getting tweet #28...\n",
      "Getting tweet #29...\n",
      "Getting tweet #30...\n",
      "Getting tweet #31...\n",
      "Getting tweet #32...\n",
      "Getting tweet #33...\n",
      "Filtering...\n",
      "Getting tweet #34...\n",
      "Getting tweet #35...\n",
      "Getting tweet #36...\n",
      "Getting tweet #37...\n",
      "Getting tweet #38...\n",
      "Getting tweet #39...\n",
      "Getting tweet #40...\n",
      "Getting tweet #41...\n",
      "Getting tweet #42...\n",
      "Getting tweet #43...\n",
      "Getting tweet #44...\n",
      "Getting tweet #45...\n",
      "Getting tweet #46...\n",
      "Getting tweet #47...\n",
      "Getting tweet #48...\n",
      "Getting tweet #49...\n",
      "Filtering...\n",
      "Getting tweet #50...\n",
      "Completed collection of tweets.\n"
     ]
    }
   ],
   "source": [
    "stream.filter(track=['anime'], languages=['en'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compile the tweet data into a DataFrame, grab out the texts and clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>entities</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>extended_tweet</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>filter_level</th>\n",
       "      <th>...</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>quoted_status_permalink</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sun Jul 15 04:15:29 +0000 2018</td>\n",
       "      <td>[14, 40]</td>\n",
       "      <td>{'urls': [], 'symbols': [], 'hashtags': [], 'u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>@BucksMachine Tfw metal gear solid anime</td>\n",
       "      <td>1531628129029</td>\n",
       "      <td>False</td>\n",
       "      <td>{'location': 'Michigan, USA', 'profile_sidebar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sun Jul 15 04:15:29 +0000 2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'urls': [{'indices': [39, 62], 'expanded_url'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://www.google.com/\" rel=\"nofollo...</td>\n",
       "      <td>I added a video to a @YouTube playlist https:/...</td>\n",
       "      <td>1531628129230</td>\n",
       "      <td>False</td>\n",
       "      <td>{'location': 'Fontana, CA', 'profile_sidebar_f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sun Jul 15 04:15:29 +0000 2018</td>\n",
       "      <td>[0, 35]</td>\n",
       "      <td>{'urls': [{'indices': [36, 59], 'expanded_url'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>1017641236293963777</td>\n",
       "      <td>{'expanded': 'https://twitter.com/EGGCHlM/stat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>What did you do to Chimmy shdksbskj https://t....</td>\n",
       "      <td>1531628129464</td>\n",
       "      <td>False</td>\n",
       "      <td>{'location': 'partly ðŸ”ž', 'profile_sidebar_fill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sun Jul 15 04:15:30 +0000 2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'urls': [{'indices': [94, 117], 'expanded_url...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://ifttt.com\" rel=\"nofollow\"&gt;IFT...</td>\n",
       "      <td>Anime Expo Opens for Fans of Japanese Culture ...</td>\n",
       "      <td>1531628130783</td>\n",
       "      <td>False</td>\n",
       "      <td>{'location': None, 'profile_sidebar_fill_color...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sun Jul 15 04:15:31 +0000 2018</td>\n",
       "      <td>[0, 69]</td>\n",
       "      <td>{'urls': [{'indices': [46, 69], 'expanded_url'...</td>\n",
       "      <td>{'media': [{'sizes': {'thumb': {'h': 150, 'res...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://publicize.wp.com/\" rel=\"nofoll...</td>\n",
       "      <td>cFreezeâ€™s Anime Watching: Week 28, 2018Â Recap ...</td>\n",
       "      <td>1531628131733</td>\n",
       "      <td>False</td>\n",
       "      <td>{'location': 'United States', 'profile_sidebar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  contributors coordinates                      created_at display_text_range  \\\n",
       "0         None        None  Sun Jul 15 04:15:29 +0000 2018           [14, 40]   \n",
       "1         None        None  Sun Jul 15 04:15:29 +0000 2018                NaN   \n",
       "2         None        None  Sun Jul 15 04:15:29 +0000 2018            [0, 35]   \n",
       "3         None        None  Sun Jul 15 04:15:30 +0000 2018                NaN   \n",
       "4         None        None  Sun Jul 15 04:15:31 +0000 2018            [0, 69]   \n",
       "\n",
       "                                            entities  \\\n",
       "0  {'urls': [], 'symbols': [], 'hashtags': [], 'u...   \n",
       "1  {'urls': [{'indices': [39, 62], 'expanded_url'...   \n",
       "2  {'urls': [{'indices': [36, 59], 'expanded_url'...   \n",
       "3  {'urls': [{'indices': [94, 117], 'expanded_url...   \n",
       "4  {'urls': [{'indices': [46, 69], 'expanded_url'...   \n",
       "\n",
       "                                   extended_entities extended_tweet  \\\n",
       "0                                                NaN            NaN   \n",
       "1                                                NaN            NaN   \n",
       "2                                                NaN            NaN   \n",
       "3                                                NaN            NaN   \n",
       "4  {'media': [{'sizes': {'thumb': {'h': 150, 'res...            NaN   \n",
       "\n",
       "   favorite_count  favorited filter_level  \\\n",
       "0               0      False          low   \n",
       "1               0      False          low   \n",
       "2               0      False          low   \n",
       "3               0      False          low   \n",
       "4               0      False          low   \n",
       "\n",
       "                         ...                         quoted_status_id_str  \\\n",
       "0                        ...                                          NaN   \n",
       "1                        ...                                          NaN   \n",
       "2                        ...                          1017641236293963777   \n",
       "3                        ...                                          NaN   \n",
       "4                        ...                                          NaN   \n",
       "\n",
       "                             quoted_status_permalink reply_count  \\\n",
       "0                                                NaN           0   \n",
       "1                                                NaN           0   \n",
       "2  {'expanded': 'https://twitter.com/EGGCHlM/stat...           0   \n",
       "3                                                NaN           0   \n",
       "4                                                NaN           0   \n",
       "\n",
       "  retweet_count  retweeted                                             source  \\\n",
       "0             0      False  <a href=\"http://twitter.com/download/android\" ...   \n",
       "1             0      False  <a href=\"https://www.google.com/\" rel=\"nofollo...   \n",
       "2             0      False  <a href=\"http://twitter.com/download/android\" ...   \n",
       "3             0      False  <a href=\"https://ifttt.com\" rel=\"nofollow\">IFT...   \n",
       "4             0      False  <a href=\"http://publicize.wp.com/\" rel=\"nofoll...   \n",
       "\n",
       "                                                text   timestamp_ms  \\\n",
       "0           @BucksMachine Tfw metal gear solid anime  1531628129029   \n",
       "1  I added a video to a @YouTube playlist https:/...  1531628129230   \n",
       "2  What did you do to Chimmy shdksbskj https://t....  1531628129464   \n",
       "3  Anime Expo Opens for Fans of Japanese Culture ...  1531628130783   \n",
       "4  cFreezeâ€™s Anime Watching: Week 28, 2018Â Recap ...  1531628131733   \n",
       "\n",
       "   truncated                                               user  \n",
       "0      False  {'location': 'Michigan, USA', 'profile_sidebar...  \n",
       "1      False  {'location': 'Fontana, CA', 'profile_sidebar_f...  \n",
       "2      False  {'location': 'partly ðŸ”ž', 'profile_sidebar_fill...  \n",
       "3      False  {'location': None, 'profile_sidebar_fill_color...  \n",
       "4      False  {'location': 'United States', 'profile_sidebar...  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tweetsToDataFrame(json_file):\n",
    "    data = []\n",
    "    with open(json_file, 'r') as json_data:\n",
    "        for line in json_data:\n",
    "            tweet = json.loads(line) # load it as Python dict\n",
    "            data.append(tweet)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "tweet_df = tweetsToDataFrame('twitter_stream_data.json')\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the tweets\n",
    "\n",
    "We can preprocess the tweets using the same process as earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['text'] = tweet_df['text'].map(clean_text)\n",
    "tweet_df = tweet_df[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sequences = tokenizer.texts_to_sequences(tweet_df['text'])\n",
    "padded_tweet_sequences = pad_sequences(tweet_sequences, maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 1s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_tweet = best_lstm_cnn_model.predict(padded_tweet_sequences, verbose=1, batch_size=2048)\n",
    "y_pred_tweet = pd.DataFrame(y_pred_tweet, columns=['prediction_prob'])\n",
    "y_pred_tweet['prediction'] = y_pred_tweet['prediction_prob'].map(lambda p: 1 if p >= 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.merge(tweet_df, y_pred_tweet, left_index=True, right_index=True, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfw metal gear solid anime | Sentiment: 1\n",
      "I added a video to a playlist THIS IS A CRAZY ANIME Reacting to Prison School | Sentiment: 1\n",
      "What did you do to Chimmy shdksbskj | Sentiment: 1\n",
      "Anime Expo Opens for Fans of Japanese Culture in Buenos Aires Latin American Herald Tribune | Sentiment: 1\n",
      "cFreeze s Anime Watching Week 28 2018 Recap | Sentiment: 1\n",
      "WOAHHHHH 95 LIKES WTH IM WATCHING ANIME AND I COME BACK TO THIS i m shook | Sentiment: 1\n",
      "im all about the emotional anime twinks | Sentiment: 0\n",
      "Binge watch some anime it ll help | Sentiment: 1\n",
      "Sailor moon crystal crunchyroll the devil is a part timer netflix your name some anime websit | Sentiment: 1\n",
      "all of the characters were individually so unique which means all of the relationships were complicated and meaning | Sentiment: 1\n",
      "Apprill At least until she turns out t | Sentiment: 0\n",
      "Deadass lost my composure bruh | Sentiment: 0\n",
      "oooo lemme give it a watch | Sentiment: 1\n",
      "jimin you re the sweetest | Sentiment: 1\n",
      "A year ago I would never put a poster up of anime cuz I was too embarrassed look where I m at now wow I m so proud of myself | Sentiment: 1\n",
      "recommend some good anime | Sentiment: 1\n",
      "I m a twin sister with a twin brother classmates made fun of me for being weird all the time and I hated the kids | Sentiment: 0\n",
      "You know niggas be posers | Sentiment: 1\n",
      "En Lucy Del anime Elfen Lied | Sentiment: 0\n",
      "Is it bad if I m staring to like anime more | Sentiment: 0\n",
      "animelove fanart Stereo Headphones with Sailor Moon Anime Print 12 Types | Sentiment: 1\n",
      "most underrated anime ive ever watched this is pure pog | Sentiment: 0\n",
      "henlo | Sentiment: 1\n",
      "mmm this is a fave this was the first anime i cried | Sentiment: 1\n",
      "Wow you fly through anime really quickly | Sentiment: 1\n",
      "Seriously shady move by Comic Con We stand by you guys at Every year NYCC offers less a | Sentiment: 0\n",
      "pour one out for the like three times ellie and i have tried to watch an anime to fit in with tl but we never finished it | Sentiment: 0\n",
      "The anime version of P5 s fireworks scene which just furthers my suspicion that the game s 2d animators forgot were | Sentiment: 0\n",
      "Just met a full squad of anime nerds on fort I joined their disc and we are all homies now | Sentiment: 1\n",
      "moss I ve mainly been reading it from time to time so I don t know how the anime is but it s a good read | Sentiment: 1\n",
      "Every time I watch too much anime I wanna get bangs | Sentiment: 0\n",
      "chan But what what anime is this I must know | Sentiment: 1\n",
      "I rewatched 5 episodes 5 26 of Monogatari Series Second Season iMAL iOS | Sentiment: 0\n",
      "Anime you really are full of surprises you even made an anime about cells It s entertaining and educational Watch Cells at work | Sentiment: 1\n",
      "my 13 year old self loved this anime when it came out but now it s an alright anime ig Never finished season 2 | Sentiment: 0\n",
      "Rip anime boys | Sentiment: 0\n",
      "YW3 is very important to the timeline of merchandise and anime There s no way they re | Sentiment: 1\n",
      "This anime is the epitome of plot armor and fan service The openings were sooo good same the anime wasn t | Sentiment: 1\n",
      "I m a little surprised that Star vs the Forces of Evil seems to be so little discussed on my side of AniTwitter com | Sentiment: 1\n",
      "ur art gets more anime everyday omg | Sentiment: 1\n",
      "North America is the largest market and the number one market that produces films It matters anime for NA market i | Sentiment: 1\n",
      "Honestly speaking this anime is so good for music and comedy for feels it is too bland Really I didn t feel much b | Sentiment: 0\n",
      "What was the anime girl looking at | Sentiment: 1\n",
      "a lot of my favorite anime are done by gainax trigger and wow hm THIS WOULD BE ONE OF THEM IF I COULD FUCKINF GET | Sentiment: 1\n",
      "How I felt when Frieza killed vegeta | Sentiment: 0\n",
      "Kiki s Delivery Service was my first intro to the world of anime Still has a special place in my heart lt 3 | Sentiment: 1\n",
      "Every night I can feel my leg and my arm and even my fingers The body I ve lost the comrades I ve lost It won t | Sentiment: 0\n",
      "yes i do even tho i haven t watched a lot i have a long as list to watch AwakeInJuly | Sentiment: 0\n",
      "it s my absolute favorite i know it s not amazing and it s just a school anime with demons and magic but u know | Sentiment: 1\n",
      "i started the anime thing but im too tired lol i skipped ones that i didnt watch or read but maybe ill do them all who knows | Sentiment: 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(tweet_df.shape[0]):\n",
    "    print('{} | Sentiment: {}'.format(tweet_df.loc[i]['text'], tweet_df.loc[i]['prediction']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
