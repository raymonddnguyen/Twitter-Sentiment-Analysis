{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Input, Dense, Embedding, MaxPooling1D\n",
    "from keras.layers import SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/twitter_sentiment_dataset.csv', encoding='latin1', usecols=['Sentiment', 'SentimentText'])\n",
    "data.columns = ['sentiment', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0                       is so sad for my APL frie...\n",
       "1          0                     I missed the New Moon trail...\n",
       "2          1                            omg its already 7:30 :O\n",
       "3          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1578614, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(tweet):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1420752,) (157862,) (1420752,) (157862,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], \n",
    "                                                    data['sentiment'], \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42,\n",
    "                                                    stratify=data['sentiment'])\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 100000\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "\n",
    "tokenizer.fit_on_texts(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lt This is the way i feel right now'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[159, 28, 9, 3, 131, 1, 110, 117, 29]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([X_train[15]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 35\n",
    "padded_train_sequences = pad_sequences(train_sequences, maxlen=MAX_LENGTH)\n",
    "padded_test_sequences = pad_sequences(test_sequences, maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   162,   356,   224],\n",
       "       [    0,     0,     0, ...,   879,  1656,   661],\n",
       "       [    0,     0,     0, ...,     0,   153,  6543],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,  1504,  1469, 26172],\n",
       "       [    0,     0,     0, ...,    55,    94,   433],\n",
       "       [    0,     0,     0, ...,   193,    13,     6]], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1420752, 35)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_rnn_model():\n",
    "    embedding_dim = 300\n",
    "    embedding_matrix = np.random.random((MAX_WORDS, embedding_dim))\n",
    "    \n",
    "    inp = Input(shape=(MAX_LENGTH, ))\n",
    "    x = Embedding(input_dim=MAX_WORDS, output_dim=embedding_dim, input_length=MAX_LENGTH, \n",
    "                  weights=[embedding_matrix], trainable=True)(inp)\n",
    "    x = SpatialDropout1D(rate=0.3)(x)\n",
    "    x = Bidirectional(GRU(units=100, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(1, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "rnn_simple_model = get_simple_rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus as pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dot': '/home/paperspace/anaconda3/envs/myconda/bin/dot',\n",
       " 'twopi': '/home/paperspace/anaconda3/envs/myconda/bin/twopi',\n",
       " 'neato': '/home/paperspace/anaconda3/envs/myconda/bin/neato',\n",
       " 'circo': '/home/paperspace/anaconda3/envs/myconda/bin/circo',\n",
       " 'fdp': '/home/paperspace/anaconda3/envs/myconda/bin/fdp',\n",
       " 'sfdp': '/home/paperspace/anaconda3/envs/myconda/bin/sfdp'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pydot.find_graphviz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(rnn_simple_model, to_file='rnn.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn.png](rnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1420752 samples, validate on 157862 samples\n",
      "Epoch 1/2\n",
      "1420752/1420752 [==============================] - 726s 511us/step - loss: 0.4401 - acc: 0.7924 - val_loss: 0.4006 - val_acc: 0.8192\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.81921, saving model to ./models/weights-improvement-01-0.8192.hdf5\n",
      "Epoch 2/2\n",
      "1420752/1420752 [==============================] - 723s 509us/step - loss: 0.3864 - acc: 0.8257 - val_loss: 0.3876 - val_acc: 0.8273\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.81921 to 0.82729, saving model to ./models/weights-improvement-02-0.8273.hdf5\n"
     ]
    }
   ],
   "source": [
    "filepath=\"./models/weights-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 2\n",
    "\n",
    "history = rnn_simple_model.fit(x=padded_train_sequences, \n",
    "                    y=y_train, \n",
    "                    validation_data=(padded_test_sequences, y_test), \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[checkpoint], \n",
    "                    epochs=epochs, \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157862/157862 [==============================] - 6s 41us/step\n"
     ]
    }
   ],
   "source": [
    "best_rnn_simple_model = load_model('./models/weights-improvement-02-{:0.4f}.hdf5'.format(checkpoint.best))\n",
    "\n",
    "y_pred_rnn_simple = best_rnn_simple_model.predict(padded_test_sequences, verbose=1, batch_size=2048)\n",
    "\n",
    "y_pred_rnn_simple = pd.DataFrame(y_pred_rnn_simple, columns=['prediction'])\n",
    "y_pred_rnn_simple['prediction'] = y_pred_rnn_simple['prediction'].map(lambda p: 1 if p >= 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printClassificationErrors(y_test, y_pred):\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('ROC AUC score: {}'.format(roc_auc_score(y_test, y_pred)))\n",
    "    print('Accuracy Score: {}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[65068 13776]\n",
      " [13488 65530]]\n",
      "Classification Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.83      0.83     78844\n",
      "          1       0.83      0.83      0.83     79018\n",
      "\n",
      "avg / total       0.83      0.83      0.83    157862\n",
      "\n",
      "ROC AUC score: 0.8272899712059488\n",
      "Accuracy Score: 0.8272921919144569\n"
     ]
    }
   ],
   "source": [
    "printClassificationErrors(y_test, y_pred_rnn_simple['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.tweepy_streaming import saveTweepyTweets\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "import json\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweepy_listener = saveTweepyTweets(time_limit=180, \n",
    "                                   num_of_tweets=20, \n",
    "                                   save_file='twitter_stream_data.json', \n",
    "                                   retweets=False)\n",
    "auth = OAuthHandler(config.CONSUMER_KEY, config.CONSUMER_SECRET)\n",
    "auth.set_access_token(config.ACCESS_TOKEN, config.ACCESS_TOKEN_SECRET)\n",
    "stream = Stream(auth=auth, listener=tweepy_listener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tweet #1...\n",
      "Getting tweet #2...\n",
      "Getting tweet #3...\n",
      "Getting tweet #4...\n",
      "Getting tweet #5...\n",
      "Getting tweet #6...\n",
      "Getting tweet #7...\n",
      "Getting tweet #8...\n",
      "Getting tweet #9...\n",
      "Getting tweet #10...\n",
      "Getting tweet #11...\n",
      "Getting tweet #12...\n",
      "Getting tweet #13...\n",
      "Getting tweet #14...\n",
      "Getting tweet #15...\n",
      "Getting tweet #16...\n",
      "Getting tweet #17...\n",
      "Getting tweet #18...\n",
      "Getting tweet #19...\n",
      "Getting tweet #20...\n",
      "Completed collection of tweets.\n"
     ]
    }
   ],
   "source": [
    "stream.filter(track=['anime'], languages=['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>entities</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>extended_tweet</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>filter_level</th>\n",
       "      <th>...</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>quoted_status_permalink</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sun Jun 24 12:40:34 +0000 2018</td>\n",
       "      <td>[0, 23]</td>\n",
       "      <td>{'hashtags': [], 'urls': [{'url': 'https://t.c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>1010545086218883073</td>\n",
       "      <td>{'url': 'https://t.co/uqoW5Yi3E0', 'expanded':...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>an art, an intellectual https://t.co/uqoW5Yi3E0</td>\n",
       "      <td>1529844034433</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 2375267126, 'id_str': '2375267126', 'na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sun Jun 24 12:40:48 +0000 2018</td>\n",
       "      <td>[14, 140]</td>\n",
       "      <td>{'hashtags': [], 'urls': [{'url': 'https://t.c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'full_text': '@ghostiesquid The statement as ...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>@ghostiesquid The statement as a whole is sema...</td>\n",
       "      <td>1529844048379</td>\n",
       "      <td>True</td>\n",
       "      <td>{'id': 591442372, 'id_str': '591442372', 'name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sun Jun 24 12:40:53 +0000 2018</td>\n",
       "      <td>[15, 140]</td>\n",
       "      <td>{'hashtags': [], 'urls': [{'url': 'https://t.c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'full_text': '@dear_hoya1991 yes i did too wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>@dear_hoya1991 yes i did too when i first star...</td>\n",
       "      <td>1529844053032</td>\n",
       "      <td>True</td>\n",
       "      <td>{'id': 966739678220181505, 'id_str': '96673967...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sun Jun 24 12:40:58 +0000 2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'urls': [{'url': 'https://t.c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"https://www.google.com/\" rel=\"nofollo...</td>\n",
       "      <td>I added a video to a @YouTube playlist https:/...</td>\n",
       "      <td>1529844058224</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 134038538, 'id_str': '134038538', 'name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Sun Jun 24 12:40:58 +0000 2018</td>\n",
       "      <td>[0, 24]</td>\n",
       "      <td>{'hashtags': [], 'urls': [{'url': 'https://t.c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>1010646015693803522</td>\n",
       "      <td>{'url': 'https://t.co/e8MnnplVAZ', 'expanded':...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>Top 10 Anime Plot Twists https://t.co/e8MnnplVAZ</td>\n",
       "      <td>1529844058836</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 947280390229594113, 'id_str': '94728039...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  contributors coordinates                      created_at display_text_range  \\\n",
       "0         None        None  Sun Jun 24 12:40:34 +0000 2018            [0, 23]   \n",
       "1         None        None  Sun Jun 24 12:40:48 +0000 2018          [14, 140]   \n",
       "2         None        None  Sun Jun 24 12:40:53 +0000 2018          [15, 140]   \n",
       "3         None        None  Sun Jun 24 12:40:58 +0000 2018                NaN   \n",
       "4         None        None  Sun Jun 24 12:40:58 +0000 2018            [0, 24]   \n",
       "\n",
       "                                            entities extended_entities  \\\n",
       "0  {'hashtags': [], 'urls': [{'url': 'https://t.c...               NaN   \n",
       "1  {'hashtags': [], 'urls': [{'url': 'https://t.c...               NaN   \n",
       "2  {'hashtags': [], 'urls': [{'url': 'https://t.c...               NaN   \n",
       "3  {'hashtags': [], 'urls': [{'url': 'https://t.c...               NaN   \n",
       "4  {'hashtags': [], 'urls': [{'url': 'https://t.c...               NaN   \n",
       "\n",
       "                                      extended_tweet  favorite_count  \\\n",
       "0                                                NaN               0   \n",
       "1  {'full_text': '@ghostiesquid The statement as ...               0   \n",
       "2  {'full_text': '@dear_hoya1991 yes i did too wh...               0   \n",
       "3                                                NaN               0   \n",
       "4                                                NaN               0   \n",
       "\n",
       "   favorited filter_level                        ...                          \\\n",
       "0      False          low                        ...                           \n",
       "1      False          low                        ...                           \n",
       "2      False          low                        ...                           \n",
       "3      False          low                        ...                           \n",
       "4      False          low                        ...                           \n",
       "\n",
       "  quoted_status_id_str                            quoted_status_permalink  \\\n",
       "0  1010545086218883073  {'url': 'https://t.co/uqoW5Yi3E0', 'expanded':...   \n",
       "1                  NaN                                                NaN   \n",
       "2                  NaN                                                NaN   \n",
       "3                  NaN                                                NaN   \n",
       "4  1010646015693803522  {'url': 'https://t.co/e8MnnplVAZ', 'expanded':...   \n",
       "\n",
       "  reply_count retweet_count  retweeted  \\\n",
       "0           0             0      False   \n",
       "1           0             0      False   \n",
       "2           0             0      False   \n",
       "3           0             0      False   \n",
       "4           0             0      False   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "1  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...   \n",
       "2  <a href=\"http://twitter.com/download/android\" ...   \n",
       "3  <a href=\"https://www.google.com/\" rel=\"nofollo...   \n",
       "4  <a href=\"http://twitter.com/download/android\" ...   \n",
       "\n",
       "                                                text   timestamp_ms  \\\n",
       "0    an art, an intellectual https://t.co/uqoW5Yi3E0  1529844034433   \n",
       "1  @ghostiesquid The statement as a whole is sema...  1529844048379   \n",
       "2  @dear_hoya1991 yes i did too when i first star...  1529844053032   \n",
       "3  I added a video to a @YouTube playlist https:/...  1529844058224   \n",
       "4   Top 10 Anime Plot Twists https://t.co/e8MnnplVAZ  1529844058836   \n",
       "\n",
       "   truncated                                               user  \n",
       "0      False  {'id': 2375267126, 'id_str': '2375267126', 'na...  \n",
       "1       True  {'id': 591442372, 'id_str': '591442372', 'name...  \n",
       "2       True  {'id': 966739678220181505, 'id_str': '96673967...  \n",
       "3      False  {'id': 134038538, 'id_str': '134038538', 'name...  \n",
       "4      False  {'id': 947280390229594113, 'id_str': '94728039...  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tweetsToDataFrame(json_file):\n",
    "    data = []\n",
    "    with open(json_file, 'r') as json_data:\n",
    "        for line in json_data:\n",
    "            tweet = json.loads(line) # load it as Python dict\n",
    "            data.append(tweet)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "tweet_df = tweetsToDataFrame('twitter_stream_data.json')\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df['text'] = tweet_df['text'].map(clean_text)\n",
    "tweet_df = tweet_df[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>an art an intellectual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The statement as a whole is semantically null ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hoya1991 yes i did too when i first started th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I added a video to a playlist Best Underrated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Top 10 Anime Plot Twists</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                             an art an intellectual\n",
       "1  The statement as a whole is semantically null ...\n",
       "2  hoya1991 yes i did too when i first started th...\n",
       "3  I added a video to a playlist Best Underrated ...\n",
       "4                           Top 10 Anime Plot Twists"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sequences = tokenizer.texts_to_sequences(tweet_df['text'])\n",
    "padded_tweet_sequences = pad_sequences(tweet_sequences, maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "20/20 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_tweet = rnn_simple_model.predict(padded_tweet_sequences, verbose=1, batch_size=2048)\n",
    "y_pred_tweet = pd.DataFrame(y_pred_tweet, columns=['prediction'])\n",
    "y_pred_tweet['prediction'] = y_pred_tweet['prediction'].map(lambda p: 1 if p >= 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.merge(tweet_df, y_pred_tweet, left_on=tweet_df.index, right_on=y_pred_tweet.index, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an art an intellectual 1\n",
      "The statement as a whole is semantically null actually There s no dial to increase or decrease the 0\n",
      "hoya1991 yes i did too when i first started the anime it s actually quite common among seiyuu you don t im 1\n",
      "I added a video to a playlist Best Underrated Anime Series To Watch 5 1\n",
      "Top 10 Anime Plot Twists 1\n",
      "LoL wut 1\n",
      "this has started as a series in germany just this month 1\n",
      "Sterl That guy rarely does anime He s more prominent in 0\n",
      "anime list blog Cutie Honey Universe12 1\n",
      "It s funny because how I like both kpop and anime but when I try to combine two things I like anime fans come to m 1\n",
      "RT TO JOIN IN ANIMEBOYISM GC W is actually a cult we worship anime boys more on filos 1\n",
      "i don t like this but i like this 0\n",
      "The anime is so baaaaaaaaaaaad 0\n",
      "Zinba Free Anime Green Screen via 1\n",
      "Boys Love Sexy Anime Pokemon Gardevoir Hentai Hugging Body Pillow Case Cover 1\n",
      "Changes XXXTentacion sad anime via 0\n",
      "Ok sorry this emoji is cringe 0\n",
      "catching up on anime is such a chore 0\n",
      "Excuse me what anime is this 1\n",
      "cant believe yula and i are having our first fight as a couple over an anime 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(tweet_df.shape[0]):\n",
    "    print(tweet_df.loc[i]['text'], tweet_df.loc[i]['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
